const WebSocket = require('ws');
const openAIService = require('../services/openAIService');
const speechService = require('../services/speechService');

const wss = new WebSocket.Server({ port: 8080 });

wss.on('connection', (ws) => {
    console.log('üì± Cliente conectado al bot ganadero');

    // Estado de la sesi√≥n de audio por conexi√≥n
    let audioBuffer = null;   // Buffer acumulador de audio
    let audioActive = false;  // Si estamos grabando
    let audioSessionId = null;
    let autoChat = false;
    let audioFormat = 'wav';

    ws.on('message', async (message) => {
        // Si hay una sesi√≥n de audio activa y el mensaje es binario, acumular audio
        if (audioActive && Buffer.isBuffer(message)) {
            if (audioBuffer) {
                audioBuffer = Buffer.concat([audioBuffer, message]);
            } else {
                audioBuffer = message;
            }
            return;
        }

        // Mensajes JSON de control
        try {
            const data = JSON.parse(message);

            // ============================================
            // MODO TEXTO: Chat normal con texto
            // ============================================
            if (data.userMessage) {
                const { callSid, userMessage } = data;
                await openAIService.completion(callSid, userMessage, ws);
                return;
            }

            // ============================================
            // MODO AUDIO: Grabaci√≥n y transcripci√≥n
            // ============================================

            // Iniciar captura de audio
            if (data.action === 'start_audio') {
                audioSessionId = data.session_id || null;
                autoChat = data.autoChat || false;
                audioFormat = data.format || 'wav'; // wav, webm, ogg, mp3
                audioBuffer = null;
                audioActive = true;

                console.log('üéôÔ∏è [WS] Captura de audio iniciada, sesi√≥n:', audioSessionId);

                ws.send(JSON.stringify({
                    event: 'speech_ready',
                    message: `Env√≠a audio en formato ${audioFormat}. Env√≠a stop_audio para transcribir.`
                }));
                return;
            }

            // Detener captura y transcribir
            if (data.action === 'stop_audio') {
                audioActive = false;
                console.log('üõë [WS] Captura de audio detenida');

                if (!audioBuffer || audioBuffer.length === 0) {
                    ws.send(JSON.stringify({
                        event: 'speech_error',
                        error: 'No se recibi√≥ audio'
                    }));
                    return;
                }

                console.log(`üì¶ [WS] Audio acumulado: ${(audioBuffer.length / 1024).toFixed(1)}KB`);

                try {
                    // Transcribir el buffer acumulado
                    const result = await speechService.transcribeBuffer(audioBuffer, audioFormat);
                    audioBuffer = null;

                    if (!result.text) {
                        ws.send(JSON.stringify({
                            event: 'speech_error',
                            error: 'No se detect√≥ habla en el audio'
                        }));
                        return;
                    }

                    console.log('üìù [WS] Texto reconocido:', result.text);

                    // Enviar transcripci√≥n al cliente
                    if (ws.readyState === WebSocket.OPEN) {
                        ws.send(JSON.stringify({
                            event: 'speech_final',
                            text: result.text,
                            duration: result.duration
                        }));
                    }

                    // Si autoChat, enviar al chatbot autom√°ticamente
                    if (autoChat && result.text) {
                        console.log('ü§ñ [WS] Enviando texto al chatbot...');
                        await openAIService.completion(audioSessionId, result.text, ws);
                    }
                } catch (error) {
                    console.error('‚ùå [WS] Error transcribiendo:', error.message);
                    ws.send(JSON.stringify({
                        event: 'speech_error',
                        error: error.message
                    }));
                }

                ws.send(JSON.stringify({
                    event: 'speech_stopped',
                    message: 'Sesi√≥n de audio finalizada'
                }));
                return;
            }

            // Enviar un audio completo de una sola vez (sin start/stop)
            if (data.action === 'transcribe_audio' && data.audio) {
                console.log('üéôÔ∏è [WS] Audio base64 recibido para transcripci√≥n');
                try {
                    const buffer = Buffer.from(data.audio, 'base64');
                    const format = data.format || 'wav';
                    const result = await speechService.transcribeBuffer(buffer, format);

                    ws.send(JSON.stringify({
                        event: 'speech_final',
                        text: result.text,
                        duration: result.duration
                    }));

                    if (data.autoChat && result.text) {
                        const sessionId = data.session_id || null;
                        await openAIService.completion(sessionId, result.text, ws);
                    }
                } catch (error) {
                    ws.send(JSON.stringify({
                        event: 'speech_error',
                        error: error.message
                    }));
                }
                return;
            }

        } catch (error) {
            if (!audioActive) {
                console.error('Error procesando mensaje:', error.message);
            }
        }
    });

    ws.on('close', () => {
        console.log('‚ùå Cliente desconectado');
        audioBuffer = null;
        audioActive = false;
    });
});

module.exports = wss;